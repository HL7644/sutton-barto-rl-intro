# -*- coding: utf-8 -*-
"""MC: Blackjack

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zxos2C80KDanV8cfvAbtsMWb7NWPcJCT
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
import random

torch.manual_seed(0)

def get_state(player_sum, dealer_showing, usable_ace):
  state=[]
  for p_idx, player in enumerate(player_sum):
    for d_idx, dealer in enumerate(dealer_showing):
      for u_ace in usable_ace:
        state.append([p_idx, d_idx, u_ace])
  
  return state

def get_init_policy(states, player_sum):
  policy=torch.zeros(len(states))
  for s_idx, state in enumerate(states):
    p_idx=state[0]
    d_idx=state[1]
    u_idx=state[2]
    player=player_sum[p_idx]
    if player>=20:
      policy[s_idx]=1
  return policy

def get_init_esoft_policy(states, player_sum, action_list, epsilon):
  policy=[]
  len_a=len(action_list)
  for s_idx, state in enumerate(states):
    policy_temp=[epsilon/len_a]*len_a
    p_idx=state[0]
    if p_idx>=20:
      policy_temp[1]+=(1-epsilon)
    else:
      policy_temp[0]+=(1-epsilon)
    policy.append(policy_temp)
  return policy

def reverse_list(list_before):
  size=len(list_before)
  list_after=[]
  for idx in range(size):
    list_after.append(list_before[size-idx-1])
  return list_after

class Blackjack():
  def __init__(self):
    self.player_sum=torch.LongTensor(np.arange(12,22,1)) #state: player's sum
    self.usable_ace=[0,1] #boolean for usable ace
    self.dealer_showing=torch.LongTensor(np.arange(1,11,1)) #one card dealer is showing
    self.state=get_state(self.player_sum, self.dealer_showing, self.usable_ace) #state of index
    self.action_list=['hit','stick'] #0idx indicating hit, 1idx indicating stick
    self.len_a=len(self.action_list)

    self.policy=get_init_policy(self.state, self.player_sum) #policy tensor based on state_index
    self.epsilon=0.1

    self.action_value=torch.zeros(len(self.state), len(self.action_list))
    self.state_value=torch.zeros(len(self.state))
    self.av_count=torch.zeros(len(self.state), len(self.action_list))
    self.sv_count=torch.zeros(len(self.state))
    self.iter=500000
    self.gamma=1.


  def find_state_idx(self, state):
    for idx, st in enumerate(self.state):
      if st[0]==state[0] and st[1]==state[1] and st[2]==state[2]:
        return idx

  def progression(self, sidx, aidx): #progression of episode based on current policy
    current=self.state[sidx]
    p_idx=current[0]
    d_idx=current[1]
    u_idx=current[2]
    if aidx==0: #hit
      next_card=np.random.randint(1,11) #if A is next card-> always count as 1
      player=self.player_sum[p_idx]+next_card
      if player<=21: #normal progression
        p_idx_f=player-12
        termination=False
        reward=0
      else: #goes bust
        if self.usable_ace[u_idx]: #if usable ace is available
          u_idx=0 #usable ace is no longer functionate
          player-=10 #convert ace from 11 to 1
          p_idx_f=player-12
          termination=False
          reward=0
        else:
          p_idx_f=p_idx
          termination=True 
          reward=-1
    else: #stick w/o bust
      p_idx_f=p_idx
      termination=True
      reward=self.dealer_progression(p_idx_f, d_idx)
    state_f=[p_idx_f, d_idx, u_idx]
    state_f_idx=self.find_state_idx(state_f)
    a_idx_f=self.policy[state_f_idx] #policy changes within iteration
    return state_f_idx, a_idx_f, reward, termination
  
  def dealer_progression(self, p_idx_f, d_idx): #case of non-bust termination
    player=self.player_sum[p_idx_f].detach().clone()
    dealer=self.dealer_showing[d_idx].detach().clone()
    while(1): #dealer's action
      next_card=np.random.randint(1,11)
      dealer+=next_card
      if dealer>=17: #hit until 17
        break
    #deciding reward
    if dealer>21: #case of dealer bust
      reward=1
    else:
      if player>dealer:
        reward=1
      elif player==dealer:
        reward=0
      else:
        reward=-1
    return reward
  
  def episode_generator(self): #following policy w/ES
    episode_sidx=[]
    episode_aidx=[]
    episode_reward=[]
    #use exploring starts
    init_sidx=np.random.randint(0,len(self.state))
    episode_sidx.append(init_sidx)
    init_aidx=np.random.randint(0,len(self.action_list))
    episode_aidx.append(init_aidx)
    #if episode terminates after initial step
    if init_aidx==1:
      termination=True
      state_temp=self.state[init_sidx]
      reward=self.dealer_progression(state_temp[0], state_temp[1])
      episode_reward.append(reward)
    #generate episode until termination
    else:
      termination=False
      while(termination==False):
        sidx=episode_sidx[-1] #prev. state
        aidx=episode_aidx[-1] #prev. action
        state_f_idx, a_idx_f, reward, termination=self.progression(sidx, aidx)
        episode_reward.append(reward)
        if termination==False:
          episode_sidx.append(state_f_idx)
          episode_aidx.append(int(a_idx_f))

    return episode_sidx, episode_aidx, episode_reward
  
  def evaluation(self):
    for epoch in range(1, self.iter+1):
      ep_sidx, ep_aidx, ep_rew=self.episode_generator()
      ep_sidx=reverse_list(ep_sidx) #access data in reversing order
      ep_aidx=reverse_list(ep_aidx)
      ep_rew=reverse_list(ep_rew)
      returns=0
      for idx, s_idx in enumerate(ep_sidx):
        a_idx=ep_aidx[idx]
        reward=ep_rew[idx]
        returns+=reward
        self.av_count[s_idx, a_idx]+=1
        self.sv_count[s_idx]+=1
        self.state_value[s_idx]=self.state_value[s_idx]+(returns-self.state_value[s_idx])/self.sv_count[s_idx]
        self.action_value[s_idx, a_idx]=self.action_value[s_idx, a_idx]+(returns-self.action_value[s_idx, a_idx])/self.av_count[s_idx, a_idx] #update on average
        #policy improvement
        #temp_tensor=torch.zeros(len(self.action_list))
        #for a_idx_temp in range(len(self.action_list)):
        #  temp_tensor[a_idx_temp]=self.action_value[s_idx, a_idx_temp]
        #argmax_idx=torch.argmax(temp_tensor, dim=0)
        #self.policy[s_idx]=argmax_idx
    return self.action_value, self.state_value, self.policy

class E_soft_Blackjack():
  def __init__(self):
    self.player_sum=torch.LongTensor(np.arange(12,22,1)) #state: player's sum
    self.usable_ace=[0,1] #boolean for usable ace
    self.dealer_showing=torch.LongTensor(np.arange(1,11,1)) #one card dealer is showing
    self.state=get_state(self.player_sum, self.dealer_showing, self.usable_ace) #state of index
    self.action_list=[0,1] #0idx indicating hit, 1idx indicating stick
    self.len_a=len(self.action_list)

    self.epsilon=0.5
    self.e_soft_policy=get_init_esoft_policy(self.state, self.player_sum, self.action_list, self.epsilon) #e-soft policy list

    self.action_value=torch.zeros(len(self.state), len(self.action_list))
    self.state_value=torch.zeros(len(self.state))
    self.av_count=torch.zeros(len(self.state), len(self.action_list))
    self.sv_count=torch.zeros(len(self.state))
    self.iter=500000
    self.gamma=1.

  def find_state_idx(self, state):
    for idx, st in enumerate(self.state):
      if st[0]==state[0] and st[1]==state[1] and st[2]==state[2]:
        return idx

  def e_soft_progression(self, sidx, aidx):
    current=self.state[sidx]
    p_idx=current[0]
    d_idx=current[1]
    u_idx=current[2]
    if aidx==0: #hit
      next_card=np.random.randint(1,11) #if A is next card-> always count as 1
      player=self.player_sum[p_idx]+next_card
      if player<=21: #normal progression
        p_idx_f=player-12
        termination=False
        reward=0
      else: #goes bust
        if self.usable_ace[u_idx]: #if usable ace is available
          u_idx=0 #usable ace is no longer functionate
          player-=10 #convert ace from 11 to 1
          p_idx_f=player-12
          termination=False
          reward=0
        else:
          p_idx_f=p_idx
          termination=True 
          reward=-1
    else: #stick w/o bust
      p_idx_f=p_idx
      termination=True
      reward=self.dealer_progression(p_idx_f, d_idx)
    state_f=[p_idx_f, d_idx, u_idx]
    state_f_idx=self.find_state_idx(state_f)
    a_idx_f=random.choices(self.action_list, self.e_soft_policy[state_f_idx]) #policy selection w/weight
    return state_f_idx, a_idx_f, reward, termination

  def dealer_progression(self, p_idx_f, d_idx): #case of non-bust termination
    player=self.player_sum[p_idx_f].detach().clone()
    dealer=self.dealer_showing[d_idx].detach().clone()
    while(1): #dealer's action
      next_card=np.random.randint(1,11)
      dealer+=next_card
      if dealer>=17: #hit until 17
        break
    #deciding reward
    if dealer>21: #case of dealer bust
      reward=1
    else:
      if player>dealer:
        reward=1
      elif player==dealer:
        reward=0
      else:
        reward=-1
    return reward

  def e_soft_episode_generator(self): #following e_soft policy w/o ES
    episode_sidx=[]
    episode_aidx=[]
    episode_reward=[]
    #use fixed start
    init_sidx=self.find_state_idx([1,1,1])
    episode_sidx.append(init_sidx)
    init_aidx=0
    episode_aidx.append(init_aidx)
    #generate episode until termination
    termination=False
    while(termination==False):
      sidx=episode_sidx[-1] #prev. state
      aidx=episode_aidx[-1] #prev. action
      state_f_idx, a_idx_f, reward, termination=self.e_soft_progression(sidx, aidx)
      episode_reward.append(reward)
      if termination==False:
        episode_sidx.append(state_f_idx)
        episode_aidx.append(int(a_idx_f[0]))

    return episode_sidx, episode_aidx, episode_reward

  def evaluation(self):
    for epoch in range(1, self.iter+1):
      ep_sidx, ep_aidx, ep_rew=self.e_soft_episode_generator()
      ep_sidx=reverse_list(ep_sidx) #access data in reversing order
      ep_aidx=reverse_list(ep_aidx)
      ep_rew=reverse_list(ep_rew)
      returns=0
      for idx, s_idx in enumerate(ep_sidx):
        a_idx=ep_aidx[idx]
        reward=ep_rew[idx]
        returns+=reward
        self.av_count[s_idx, a_idx]+=1
        self.sv_count[s_idx]+=1
        self.state_value[s_idx]=self.state_value[s_idx]+(returns-self.state_value[s_idx])/self.sv_count[s_idx]
        self.action_value[s_idx, a_idx]=self.action_value[s_idx, a_idx]+(returns-self.action_value[s_idx, a_idx])/self.av_count[s_idx, a_idx] #update on average
        #policy improvement
        temp_tensor=torch.zeros(len(self.action_list))
        for a_idx_temp in range(len(self.action_list)):
          temp_tensor[a_idx_temp]=self.action_value[s_idx, a_idx_temp]
        argmax_idx=torch.argmax(temp_tensor, dim=0)
        self.e_soft_policy[s_idx]=[self.epsilon/self.len_a]*self.len_a
        self.e_soft_policy[s_idx][argmax_idx]+=1-self.epsilon
    return self.action_value, self.state_value, self.e_soft_policy

blackjack=Blackjack()
action_value, state_value, policy=blackjack.evaluation()

s_idx=blackjack.find_state_idx([1,1,1])
print(state_value[23], action_value[23,0], action_value[23,1])

usable_ace_plot=plt.axes(projection='3d')
x=blackjack.player_sum
y=blackjack.dealer_showing
X,Y=torch.meshgrid(x,y, indexing='ij')
leny, lenx=X.size()
Z=torch.zeros(lenx, leny)
for x_idx in range(lenx):
  for y_idx in range(leny):
    state=[x_idx, y_idx, 1]
    s_idx=blackjack.find_state_idx(state)
    Z[x_idx, y_idx]=state_value[s_idx]
usable_ace_plot.plot_surface(X.numpy(), Y.numpy(), Z.numpy())
usable_ace_plot.view_init(30,240)
plt.show()

non_ace_plot=plt.axes(projection='3d')
x=blackjack.player_sum
y=blackjack.dealer_showing
X,Y=torch.meshgrid(x,y, indexing='ij')
leny, lenx=X.size()
Z=torch.zeros(lenx, leny)
for x_idx in range(lenx):
  for y_idx in range(leny):
    state=[x_idx, y_idx, 0]
    s_idx=blackjack.find_state_idx(state)
    Z[x_idx, y_idx]=state_value[s_idx]
non_ace_plot.plot_surface(X.numpy(), Y.numpy(), Z.numpy())
non_ace_plot.view_init(30,240)
plt.show()