# -*- coding: utf-8 -*-
"""DP Gridworld: Policy Evaluation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ftu6gjOdeWwNIh9isOc4feFltgzmJkv1
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
import random

torch.manual_seed(0)

#equiprobable policy pi(a|s)=0.25-> compute value, follow greedy
class gridworld():
  def __init__(self):
    self.state_value=torch.normal(torch.full([5,5],0.), torch.full([5,5],1.))
    #self.state_value=torch.zeros(5,5)
    self.actions=torch.LongTensor([[0,-1],[0,1],[-1,0],[1,0]]) #each corresponding to left, right, up, left
    self.A=torch.LongTensor([0,1])
    self.A_prime=torch.LongTensor([4,1])
    self.B=torch.LongTensor([0,3])
    self.B_prime=torch.LongTensor([2,3])
    self.gamma=0.9

  def get_policy(self, action, state): #computing pi(a|s)
    return 0.25
  
  def final_state(self, state, action): #computing s' for (s,a)
    if state[0]==0 and state[1]==1: #if state is A
      state_f=self.A_prime
    elif state[0]==0 and state[1]==3: #if state is B
      state_f=self.B_prime
    else:
      state_f=torch.add(state,action)
      if state_f[0]>4 or state_f[0]<0 or state_f[1]>4 or state_f[1]<0: #off the grid
        state_f=state #return to prev. state
        
    return state_f

  def reward(self, state, action, state_f): #compute reward for each action
    if torch.equal(state, state_f): #check if action makes it go off grid
      reward=-1
    
    if state[0]==0 and state[1]==1: #special cases
      reward=10
    elif state[0]==0 and state[1]==3:
      reward=5
    else: #general cases
      reward=0
    return reward

  def dynamics(self, state_f, reward, state, action): #computing p(s',r|s,a)
    return 1 #in this problem: state_f and reward is unique given state and action

  def policy_evaluation(self): #update state-value using iteration
    thresh=0.0001
    error=1
    count=0
    while error>=thresh:
      error=0
      for x in range(5):
        for y in range(5): #for all states
          temp=0
          v=self.state_value.detach().clone()[x,y]
          state=torch.LongTensor([x,y])
          for action in self.actions:
            state_f=self.final_state(state, action)
            reward=self.reward(state, action, state_f)
            p=self.dynamics(state_f, reward, state, action)
            pi=self.get_policy(action, state)
            temp+=pi*p*(reward+self.gamma*self.state_value[state_f[0],state_f[1]])
          self.state_value[x,y]=temp
          error=max(error, torch.abs(v-self.state_value[x,y]))
      count+=1
    return self.state_value, count

grid=gridworld()
state_value, count=grid.policy_evaluation()
print(state_value)
print(count)