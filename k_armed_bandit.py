# -*- coding: utf-8 -*-
"""k-Armed Bandit

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ygTAzhCSm23qItpicJvcQ6P_tAFqFEpV
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
import random

torch.manual_seed(0)

class k_armed_bandit():
  def __init__(self, k, mean, std, step_size, initial_value, unbias):
    self.k=k
    self.mean=mean
    self.std=std
    self.step_size=step_size
    self.trace=step_size
    self.unbias=unbias
    self.action_value=torch.normal(torch.full([self.k], self.mean), torch.full([self.k], self.std))
    #initialize estimates & preferences for gradient
    self.est_action_value=torch.full([self.k], initial_value)

    self.count=torch.full([self.k], 0.).long()
    self.time=1

  def update_value(self, choice, count):
    #use incremental method
    a_value=self.action_value[choice]
    #give reward
    reward=torch.normal(torch.FloatTensor([a_value]), torch.FloatTensor([1]))
    #update
    if self.unbias:
      self.trace=self.step_size*(1-self.trace)+self.trace
      unbiased_step_size=self.step_size/self.trace
      self.est_action_value[choice]=a_value+unbiased_step_size*(a_value-reward[0])
    else:
      self.est_action_value[choice]=a_value+self.step_size*(a_value-reward[0])
    #change action value (to make non-stationary)
    increment=torch.normal(torch.full([self.k], 0.), torch.full([self.k], 0.01))
    self.action_value+=increment

  def greedy(self, ucb, c):
    if ucb:
      zero_flag=0
      ucb_estimate=torch.zeros([self.k])
      #generate estimate using ucb
      for i in range(self.k):
        if self.count[i]==0:
          zero_flag=i+1
          break
        else:
          ucb_estimate[i]=self.est_action_value[i]+c*np.sqrt(np.log(self.time)/self.count[i])
      #if denominator is zero
      if zero_flag!=0:
        choice=torch.argmax(ucb_estimate, dim=0)
      else:
        choice=zero_flag-1
    else:
      choice=torch.argmax(self.est_action_value, dim=0)
    self.update_value(choice, self.count)
    #add +1 after update
    self.count[choice]+=1
    self.time+=1

  def non_greedy(self):
    choice=torch.randint(low=0, high=self.k, size=[1])[0]
    self.update_value(choice, self.count)
    #add +1 after update
    self.count[choice]+=1
    self.time+=1

class gradient_bandit():
  def __init__(self, k, mean, std, step_size):
    self.k=k
    self.step_size=step_size
    self.action_value=torch.normal(torch.full([self.k], mean), torch.full([self.k], std))

    self.preference=torch.full([self.k], 4.)
    self.baseline=0
    self.time=1

  def update_pref(self, choice):
    #get reward
    reward=torch.normal(torch.FloatTensor([self.action_value[choice]]), torch.FloatTensor([1]))
    #get probability
    probability=F.softmax(self.preference, dim=0)
    #update preference
    for i in range(self.k):
      if i==choice:
        self.preference[i]=self.preference[i]+self.step_size*(reward-self.baseline)*(1-float(probability[i]))
      else:
        self.preference[i]=self.preference[i]-self.step_size*(reward-self.baseline)*float(probability[i])
    #update baseline
    self.baseline=self.baseline+(reward-self.baseline)/self.time

  def greedy(self):
    choice=torch.argmax(self.preference, dim=0)
    self.update_pref(choice)
    self.time+=1
  
  def non_greedy(self):
    choice=torch.randint(low=0, high=self.k, size=[1])[0]
    self.update_pref(choice)
    self.time+=1
  
  def evaluate(self):

iter=1000

#e-greedy w/unbiased step_size
bandit=k_armed_bandit(k=10, mean=0., std=1., step_size=0.1, initial_value=0., unbias=True)
e=0.1
for epoch in range(1,iter+1):
  rand=random.random()
  if rand<e:
    bandit.non_greedy()
  else:
    bandit.greedy(ucb=False, c=2.)
print("e-greedy")
print(bandit.est_action_value)
print(bandit.action_value)

#optimistic initial & e-greedy
bandit=k_armed_bandit(k=10, mean=0., std=1., step_size=0.1, initial_value=5., unbias=False)
e=0.1
for epoch in range(1,iter+1):
  rand=random.random()
  if rand<e:
    bandit.non_greedy()
  else:
    bandit.greedy(ucb=False, c=2.)
print("optimistic initialization")
print(bandit.est_action_value)
print(bandit.action_value)

#UCB & e-greedy
bandit=k_armed_bandit(k=10, mean=0., std=1., step_size=0.1, initial_value=5., unbias=False)
e=0.1
for epoch in range(1,iter+1):
  rand=random.random()
  if rand<e:
    bandit.non_greedy()
  else:
    bandit.greedy(ucb=True, c=2.)
print("UCB")
print(bandit.est_action_value)
print(bandit.action_value)

#gradient bandit
bandit=gradient_bandit(k=10, mean=0., std=1., step_size=0.1)
e=0.1
for epoch in range(1,iter+1):
  rand=random.random()
  if rand<e:
    bandit.non_greedy()
  else:
    bandit.greedy()
print("Gradient")
print(bandit.preference)
print(bandit.action_value)