# -*- coding: utf-8 -*-
"""nTD: Windy Gridworld

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eyipE699QvoX4-kk_IZKtBwZnZHL8DB_
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
import random

torch.manual_seed(0)

def get_wind(grid):
  grid[:,3:6]=-1
  grid[:,6:8]=-2
  grid[:,8]=-1
  return grid

def get_state(grid):
  h,w=grid.size()
  states=[]
  for row in range(h):
    for col in range(w):
      states.append([row, col])
  return states

class windy_gridworld():
  def __init__(self, action_list):
    #get states & wind
    self.grid=torch.zeros(7,10) #grid cells
    self.states=get_state(self.grid)
    self.len_s=len(self.states)
    self.wind=get_wind(self.grid) #upwards wind
    self.start=[3,0]
    self.start_idx=self.find_s_idx(self.start)
    self.goal=[3,7]
    self.goal_idx=self.find_s_idx(self.goal)
    #get actions
    self.action_list=action_list #each corresponding to up, down, left, right
    self.len_a=len(self.action_list)
    self.a_idx_list=np.arange(0,self.len_a,1)
    self.avail_actions_idx=self.get_avail_action_idx()
    #values
    self.action_value=torch.zeros(self.len_s, self.len_a)
    #policies
    self.epsilon=0.1
    self.target_policy, self.behav_policy=self.get_policy(False)
    #iteration parameters
    self.n=3
    self.iter=1000
    self.step_size=0.01
    self.gamma=1.

  def get_avail_action_idx(self):
    avail_actions_idx=[]
    for st in self.states:
      temp=[]
      wind=self.wind[st[0],st[1]]
      for a_idx, a in enumerate(self.action_list):
        if a[0]==0 and a[1]==0:
          if wind!=0:
            temp.append(a_idx)
        else:
          st_f0=st[0]+a[0]
          st_f1=st[1]+a[1]
          if st_f0>=0 and st_f1>=0 and st_f0<self.grid.size(0) and st_f1<self.grid.size(1):
            temp.append(a_idx)
      avail_actions_idx.append(temp)
    return avail_actions_idx

  def find_s_idx(self, state):
    for s_idx, st in enumerate(self.states):
      if state[0]==st[0] and state[1]==st[1]:
        return s_idx
  
  def get_policy(self, greedy): #target: e-greedy or greedy w.r.t Q, behav: equiprobable
    target_policy=[]
    behav_policy=[]
    for s_idx, st in enumerate(self.states):
      avail_actions_idx=self.avail_actions_idx[s_idx]
      len_av=len(avail_actions_idx)
      temp=torch.zeros(len_av)
      target_temp=[0]*self.len_a
      behav_temp=[0]*self.len_a
      for idx, a_idx in enumerate(avail_actions_idx):
        if greedy:
          target_temp[a_idx]=self.epsilon/len_av
        behav_temp[a_idx]=1/len_av
        temp[idx]=self.action_value[s_idx, a_idx]
      argmax_idx=avail_actions_idx[torch.argmax(temp, dim=0)] #argmax action idx
      if greedy:
        target_temp[argmax_idx]=1
      else:
        target_temp[argmax_idx]+=(1-self.epsilon)
      target_policy.append(target_temp)
      behav_policy.append(behav_temp)
    return target_policy, behav_policy
  
  def progression(self, s_idx, a_idx):
    state=self.states[s_idx]
    action=self.action_list[a_idx]
    wind=self.wind[state[0],state[1]]
    sf0=state[0]+action[0]+wind
    sf1=state[1]+action[1]
    if sf0<=0: #wind cannot move agent out of grid
      sf0=0
    state_f=[sf0,sf1]
    s_idx_f=self.find_s_idx(state_f)
    if s_idx_f==self.goal_idx:
      termination=True
      reward=-1
    else:
      termination=False
      reward=-1
    return s_idx_f, reward, termination

  def e_greedy_update(self, s_idx):
    av_action_idx=self.avail_actions_idx[s_idx]
    len_av=len(av_action_idx)
    temp=torch.zeros(len_av)
    policy_temp=[0]*self.len_a
    for idx, a_idx in enumerate(av_action_idx):
      policy_temp[a_idx]=self.epsilon/len_av
      temp[idx]=self.action_value[s_idx,a_idx]
    max_a_idx=av_action_idx[torch.argmax(temp, dim=0)]
    policy_temp[max_a_idx]+=(1-self.epsilon)
    self.target_policy[s_idx]=policy_temp #update target policy w/action index for e-greedy
    return

  def greedy_update(self, s_idx):
    av_action_idx=self.avail_actions_idx[s_idx]
    len_av=len(av_action_idx)
    temp=torch.zeros(len_av)
    policy=[0]*self.len_a
    for idx, a_idx in enumerate(av_action_idx):
      temp[idx]=self.action_value[s_idx,a_idx]
    max_a_idx=av_action_idx[torch.argmax(temp, dim=0)]
    policy[max_a_idx]=1
    self.target_policy[s_idx]=policy
    return max_a_idx

  def n_step_sarsa(self):
    #initialize action_value & policy
    self.action_value=torch.zeros(self.len_s, self.len_a)
    self.target_policy, self.behav_policy=self.get_policy(False) #false indicating e-greedy
    for epoch in range(1, self.iter+1):
      ep_sidx=[]
      ep_aidx=[]
      ep_reward=[]
      s_idx=self.start_idx
      ep_sidx.append(s_idx)
      a_idx=random.choices(self.a_idx_list, self.target_policy[s_idx])[0]
      ep_aidx.append(a_idx)
      ep_reward.append(0) #append zero for indicing t+1 for (st,at)
      termin_step=10000000000 #large number
      step=0
      while True:
        if step<termin_step:
          s_idx=ep_sidx[-1]
          a_idx=ep_aidx[-1]
          s_idx_f, reward, termination=self.progression(s_idx, a_idx)
          ep_reward.append(reward)
          if termination==False:
            ep_sidx.append(s_idx_f)
            a_idx_f=random.choices(self.a_idx_list, self.target_policy[s_idx_f])[0]
            ep_aidx.append(a_idx_f)
          else:
            termin_step=step+1
        update_step=step-(self.n-1)
        if update_step>=0:
          n_step_return=0
          for i in range(update_step+1, min(update_step+self.n, termin_step)+1): #iterations for t+1
            n_step_return+=self.gamma**(i-(update_step+1))*ep_reward[i]
          if update_step+self.n<termin_step:
            n_step_return+=self.gamma**self.n*self.action_value[ep_sidx[step+1],ep_aidx[step+1]]
          self.action_value[ep_sidx[update_step],ep_aidx[update_step]]+=self.step_size*(n_step_return-self.action_value[ep_sidx[update_step],ep_aidx[update_step]])
          self.e_greedy_update(ep_sidx[update_step])
        if update_step==termin_step-1:
          break
        else:
          step+=1
    return self.action_value
  
  def off_policy_isr(self):
    #initialize action_value & policy
    self.action_value=torch.zeros(self.len_s, self.len_a)
    self.target_policy, self.behav_policy=self.get_policy(True) #true indicating greedy
    for epoch in range(1, self.iter+1):
      ep_sidx=[]
      ep_aidx=[]
      ep_reward=[]
      ep_isr=[]
      s_idx=self.start_idx
      ep_sidx.append(s_idx)
      a_idx=random.choices(self.a_idx_list, self.behav_policy[s_idx])[0] #follow behav policy
      ep_aidx.append(a_idx)
      ep_reward.append(0) #append zero for indicing t+1 for (st,at)
      ep_isr.append(1) #append 1 for ISR: meaning first isr is neglected (b.c action value is estimated)
      termin_step=10000000000 #large number
      step=0
      while True:
        if step<termin_step:
          s_idx=ep_sidx[-1]
          a_idx=ep_aidx[-1]
          s_idx_f, reward, termination=self.progression(s_idx, a_idx)
          ep_reward.append(reward)
          if termination==False:
            ep_sidx.append(s_idx_f)
            a_idx_f=random.choices(self.a_idx_list, self.behav_policy[s_idx_f])[0]
            ep_aidx.append(a_idx_f)
            isr=(self.target_policy[s_idx_f][a_idx_f]/self.behav_policy[s_idx_f][a_idx_f])
            ep_isr.append(isr)
          else:
            termin_step=step+1
        update_step=step-(self.n-1)
        if update_step>=0:
          isr=1
          for i in range(update_step+1, min(update_step+self.n, termin_step)): #iterations from t+1 until t+n-1 or T-1
            isr*=ep_isr[i]
          n_step_return=0
          for i in range(update_step+1, min(update_step+self.n, termin_step)+1): #iterations for t+1
            n_step_return+=self.gamma**(i-(update_step+1))*ep_reward[i]
          if update_step+self.n<termin_step:
            n_step_return+=self.gamma**self.n*self.action_value[ep_sidx[step+1],ep_aidx[step+1]]
          self.action_value[ep_sidx[update_step],ep_aidx[update_step]]+=self.step_size*isr*(n_step_return-self.action_value[ep_sidx[update_step],ep_aidx[update_step]])
          _=self.greedy_update(ep_sidx[update_step])
        if update_step==termin_step-1:
          break
        else:
          step+=1
    return self.action_value
    
  def get_trajectory(self):
    #follow greedy w.r.t action value for optimal trajectory
    state_list=[]
    sidx_list=[]
    sidx_list.append(self.start_idx)
    termination=False
    while termination==False:
      s_idx=sidx_list[-1]
      a_idx=self.greedy_update(s_idx)
      s_idx_f, _, termination=self.progression(s_idx, a_idx)
      sidx_list.append(s_idx_f)
    traj_grid=torch.zeros(7,10)
    for s_idx in sidx_list:
      state=self.states[s_idx]
      traj_grid[state[0],state[1]]=1
      state_list.append(self.states[s_idx])
    return state_list, traj_grid

action_list_normal=[[-1,0],[1,0],[0,-1],[0,1]]
gw=windy_gridworld(action_list_normal)
#action_value1=gw.n_step_sarsa()
action_value2=gw.off_policy_isr()

print(action_value2)

sl, grid=gw.get_trajectory()
print(grid)