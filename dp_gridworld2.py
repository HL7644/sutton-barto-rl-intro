# -*- coding: utf-8 -*-
"""DP Gridworld2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BJh-EFGJNSjEzeln2ZwKTdhMCxoCIBMK
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
import random

torch.manual_seed(0)

class gridworld():
  def __init__(self):
    self.state_value=torch.zeros(4,4)
    self.actions=torch.LongTensor([[0,-1],[0,1],[-1,0],[1,0]]) #left, right, up, down actions
    self.action_value=torch.zeros(4,4,4)
    self.terminal1=torch.LongTensor([0,0])
    self.terminal2=torch.LongTensor([3,3])
    self.gamma=1 #undiscounted

  def policy(self, action, state):
    return 0.25

  def get_state_f(self, state, action):
    state_f=torch.add(state, action)
    if state_f[0]>3 or state_f[0]<0 or state_f[1]>3 or state_f[1]<0: #off the grid
      state_f=state #return to original
    return state_f

  def get_reward(self, state_f, action, state):
    return -1

  def dynamics(self, state_f, reward, state, action):
    return 1
  
  def evaluation(self): #evaluate state-value ftn from given policy(stochastic)
    error=1
    thresh=0.001
    count=0
    while error>=thresh:
      error=0
      for x in range(4):
        for y in range(4):
          state=torch.LongTensor([x,y])
          if torch.equal(state, self.terminal1) or torch.equal(state, self.terminal2):
            temp=0
          else:
            v=self.state_value.detach().clone()[x,y]
            temp=0
            for action in self.actions:
              pi=self.policy(action, state)
              state_f=self.get_state_f(state, action)
              reward=self.get_reward(state_f, action, state)
              p=self.dynamics(state_f, reward, state, action)
              temp+=pi*p*(reward+self.gamma*self.state_value[state_f[0],state_f[1]])
            self.state_value[x,y]=temp
            error=max(error, torch.abs(v-temp))
            
      count+=1
    return self.state_value, count

  def get_action_value(self): #from given state-value function
    for x in range(4):
      for y in range(4):
        state=torch.LongTensor([x,y])
        if torch.equal(state, self.terminal1) or torch.equal(state, self.terminal2):
          temp=0
        else:
          for idx in range(self.actions.size(0)):
            action=self.actions[idx]
            state_f=self.get_state_f(state, action)
            reward=self.get_reward(state_f, action, state)
            p=self.dynamics(state_f, reward, state, action)
            self.action_value[x,y,idx]=p*(reward+self.gamma*self.state_value[state_f[0],state_f[1]])
    return self.action_value

  def improvement(self): #determine from given action value ftn
    optimal_action_idx=torch.zeros(4,4) #optimal action index
    for x in range(4):
      for y in range(4):
        state=torch.LongTensor([x,y])
        if torch.equal(state, self.terminal1) or torch.equal(state, self.terminal2):
          temp=0
        else:
          optimal_index=torch.argmax(self.action_value[x,y])
          optimal_action_idx[x,y]=optimal_index
    return optimal_action_idx

grid=gridworld()
state_value, count=grid.evaluation()
print(state_value)
action_value=grid.get_action_value()
print(action_value[1,1])
optimal_action_idx=grid.improvement()
print(optimal_action_idx)