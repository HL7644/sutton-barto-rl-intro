# -*- coding: utf-8 -*-
"""RL: PG Methods

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IYCp0x0fEpYUQsdXwDUvjVPh1-gSqC_b
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
import random

import json
import pandas as pd
import os.path
import xml.etree.ElementTree as ET
import PIL
from google.colab import drive

import itertools
from itertools import product

torch.manual_seed(0)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

class Short_Corridor():
  def __init__(self):
    self.s_idx_list=[0,1,2]
    self.len_s=len(self.s_idx_list)
    self.a_idx_list=[0,1] #left, right
    self.len_a=len(self.a_idx_list)

    self.len_f=2
    #linear preference
    self.theta_vector=torch.normal(torch.zeros(self.len_f).to(device), torch.ones(self.len_f)).to(device)
    #linear value approx
    self.weight=torch.normal(torch.zeros(self.len_f).to(device), torch.ones(self.len_f)).to(device)
  
  def progression(self, s_idx, a_idx):
    termination=False
    if s_idx==0: #first state
      if a_idx==0: #left
        s_idx_f=0
      else: #right
        s_idx_f=1
    elif s_idx==1: #second state
      if a_idx==0: #left
        s_idx_f=2
      else: #right
        s_idx_f=0
    elif s_idx==2: #third state
      if a_idx==0: #left
        s_idx_f=1
      else: #right -> termination
        s_idx_f=2 
        termination=True
    reward=-1
    return s_idx_f, reward, termination
  
  def get_feature_vector(self, s_idx, a_idx):
    if a_idx==0: #left
      feature_vector=torch.FloatTensor([1,0]).to(device)
    else: #right
      feature_vector=torch.FloatTensor([0,1]).to(device)
    return feature_vector
  
  def get_policy(self, s_idx):
    pref=torch.zeros(self.len_a).to(device)
    for idx, a_idx in enumerate(self.a_idx_list):
      pref[idx]=torch.dot(self.theta_vector, self.get_feature_vector(s_idx, a_idx))
    prob=F.softmax(pref, dim=0)
    a_idx=random.choices(self.a_idx_list, prob.clone().detach().cpu().numpy())[0]
    return prob, a_idx
  
  def get_state_value(self, s_idx):
    state_value=0
    prob, _=self.get_policy(s_idx)
    for a_idx, p in enumerate(prob):
      feature_vector=self.get_feature_vector(s_idx, a_idx)
      state_value=state_value+p*torch.dot(self.weight, feature_vector)
    return state_value

  def episode_generator(self):
    ep_sidx=[]
    ep_aidx=[]
    ep_reward=[]
    ep_reward.append(0) #to match indices
    s_idx=0 #start at state 0
    ep_sidx.append(s_idx)
    while True:
      _, a_idx=self.get_policy(s_idx)
      ep_aidx.append(a_idx)
      s_idx_f, reward, termination=self.progression(s_idx, a_idx)
      ep_reward.append(reward)
      if termination:
        break
      else:
        ep_sidx.append(s_idx_f)
        s_idx=s_idx_f
    return ep_sidx, ep_aidx, ep_reward

class REINFORCE():
  #with baseline
  def __init__(self):
    self.agent=Short_Corridor()
  
  def train(self, iter, w_step_size, th_step_size):
    gamma=1.
    for epoch in range(1, iter+1):
      ep_sidx, ep_aidx, ep_reward=self.agent.episode_generator()
      len_ep=len(ep_aidx)
      full_return=sum(ep_reward)
      for step in range(len_ep):
        s_idx=ep_sidx[step]
        a_idx=ep_aidx[step]
        prob, _=self.agent.get_policy(s_idx)
        #update weight
        V=self.agent.get_state_value(s_idx)
        fv=self.agent.get_feature_vector(s_idx, a_idx)
        MC_error=full_return-V
        self.agent.weight=self.agent.weight+w_step_size*(gamma**step)*MC_error*fv
        #get eligibility vector
        elig_vector=fv.clone().detach()
        #print(elig_vector)
        for a_idx_b, p in enumerate(prob):
          elig_vector=elig_vector-p*self.agent.get_feature_vector(s_idx, a_idx_b)
        #print(elig_vector)
        self.agent.theta_vector=self.agent.theta_vector+th_step_size*(gamma**step)*MC_error*elig_vector
        #update full-return
        full_return-=ep_reward[step]
      if epoch%(iter/20)==0:
        perf=self.test()
        prob, _=self.agent.get_policy(0)
        print("Epoch: {:d}, Performance: {:.3f}, R Prob: {:.2f}".format(epoch, perf, prob[1]))
    return self.agent.theta_vector
  
  def test(self):
    #get first state's value
    perf=self.agent.get_state_value(0)
    return perf

rf=REINFORCE()
theta=rf.train(iter=15000, w_step_size=0.5**(6), th_step_size=0.5**(9))
#perf=rf.test()

print(rf.agent.theta_vector)
prob, _=rf.agent.get_policy(0)
print(prob)
#approaches deterministic policy (instead of e-greedy)