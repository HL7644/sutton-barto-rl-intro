# -*- coding: utf-8 -*-
"""RL: Baird's Counterexample

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11zSpUrhvrwK5qYYf2BWbqNZqgs1qUf6-
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
import random

import json
import pandas as pd
import os.path
import xml.etree.ElementTree as ET
import PIL
from google.colab import drive

import itertools
from itertools import product

torch.manual_seed(0)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

class Baird():
  def __init__(self):
    self.s_idx_list=np.arange(0,7,1)
    self.a_idx_list=[0,1] #0: dashed, 1: soild
    self.len_s=len(self.s_idx_list)
    self.len_a=len(self.a_idx_list)

    #weight
    self.weight=torch.full([8], 1.).to(device)
  
  def weight_init(self):
    #initialize weight
    self.weight=torch.full([8], 1.).to(device)
    self.weight[6]=10
    return
  
  #return probability and accordingly selected a_idx
  def target_policy(self, s_idx):
    policy=[0,1] #only takes solid actions
    a_idx=random.choices(self.a_idx_list, policy)[0]
    return policy, a_idx

  def behavior_policy(self, s_idx):
    policy=[6/7, 1/7] #dasehd: 6/7, solid: 1/7
    a_idx=random.choices(self.a_idx_list, policy)[0]
    return policy, a_idx
  
  def get_feature_vector(self, s_idx):
    feature_vector=torch.zeros(8).to(device)
    if s_idx==6:
      feature_vector[7]=2
      feature_vector[s_idx]=1
    else:
      feature_vector[7]=1
      feature_vector[s_idx]=2
    return feature_vector
  
  def get_state_value(self, s_idx):
    feature_vector=self.get_feature_vector(s_idx)
    state_value=torch.dot(self.weight, feature_vector)
    return state_value

  def progression(self, s_idx, a_idx):
    upper_s_idx=np.arange(0,6,1)
    if a_idx==0: #dashed move
      #randomly to upper 6 states
      s_idx_f=random.choice(upper_s_idx)
      reward=0
    else: #soild move
      s_idx_f=6 #moves to lower state
      reward=0
    return s_idx_f, reward
  
  def TD_train(self):
    self.weight_init()
    #start at
    s_idx=0
    N_steps=1000
    TD_step_size=1e-2
    gamma=.99
    for step in range(1, N_steps+1):
      #print(s_idx)
      behav_prob, a_idx=self.behavior_policy(s_idx)
      target_prob, _=self.target_policy(s_idx)
      isr=target_prob[a_idx]/behav_prob[a_idx]

      s_idx_f, reward=self.progression(s_idx, a_idx)
      V=self.get_state_value(s_idx)
      V_f=self.get_state_value(s_idx_f)
      TD_error=reward+V_f-V
      feature_vector=self.get_feature_vector(s_idx)
      self.weight=self.weight+TD_step_size*isr*TD_error*feature_vector
      s_idx=s_idx_f
      if step%(N_steps/20)==0:
        print("Step: {:d}, Weight Vector Sum: {:.3f}".format(step, torch.sum(self.weight).item()))
    return self.weight
  
  def DP_train(self):
    #initialize weight
    self.weight_init()
    #start at
    s_idx=0
    N_sweeps=1000
    step_size=1e-2
    gamma=.99
    for sweep in range(1, N_sweeps+1):
      target=0
      #uniform update
      for s_idx in self.s_idx_list:
        target_prob, a_idx=self.target_policy(s_idx)
        V=self.get_state_value(s_idx)
        DP_target=0
        for ai, prob in enumerate(target_prob):
          sif, rew=self.progression(s_idx, ai)
          DP_target+=prob*(rew+gamma*self.get_state_value(sif))
        #update
        feature_vector=self.get_feature_vector(s_idx)
        target+=((DP_target-V)*feature_vector)
        #print(target)
      self.weight=self.weight+step_size/self.len_s*target
      if sweep%(N_sweeps/20)==0:
        print("Sweep: {:d}, Weight Vector Sum: {:.3f}".format(sweep, torch.sum(self.weight).item()))

    return self.weight
  
  def on_policy_DP(self):
    self.weight_init()
    #start state
    s_idx=0
    N_steps=1000
    step_size=1e-2
    gamma=.99
    for step in range(1, N_steps+1):
      target_prob, a_idx=self.target_policy(s_idx)
      #DP target
      DP_target=0
      for ai, prob in enumerate(target_prob):
        sif, rew=self.progression(s_idx, ai)
        DP_target+=prob*(rew+gamma*self.get_state_value(sif))
      #weight update
      V=self.get_state_value(s_idx)
      feature_vector=self.get_feature_vector(s_idx)
      self.weight=self.weight+step_size*(DP_target-V)*feature_vector
      #actual progression
      s_idx_f, reward=self.progression(s_idx, a_idx)
      s_idx=s_idx_f
      if step%(N_steps/20)==0:
        print("Step: {:d}, Weight Vector Sum: {:.3f}".format(step, torch.sum(self.weight).item()))
    return self.weight

baird=Baird()
TD_weight=baird.TD_train()
print(TD_weight)

baird=Baird()
DP_weight=baird.DP_train()
print(DP_weight)

baird=Baird()
op_DP_weight=baird.on_policy_DP()
print(op_DP_weight)