# -*- coding: utf-8 -*-
"""MC: Blackjack-Off Policy

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tyvimi-gO1PU_ENZZ9wpIfJaK96cj10U
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
import random

torch.manual_seed(0)

def get_state(player_sum, dealer_showing, usable_ace):
  state=[]
  for p_idx, player in enumerate(player_sum):
    for d_idx, dealer in enumerate(dealer_showing):
      for u_ace in usable_ace:
        state.append([p_idx, d_idx, u_ace])
  
  return state

def get_init_policy(states, player_sum):
  policy=torch.zeros(len(states))
  for s_idx, state in enumerate(states):
    p_idx=state[0]
    d_idx=state[1]
    u_idx=state[2]
    player=player_sum[p_idx]
    if player>=20:
      policy[s_idx]=1
  return policy

def get_behavior_policy(states, player_sum, action_list):
  #get equiprobable stochastic behavior policy
  b_policy=[]
  for s_idx, state in enumerate(states):
    b_policy.append([1/len(action_list)]*len(action_list))
  return b_policy

def reverse_list(list_before):
  size=len(list_before)
  list_after=[]
  for idx in range(size):
    list_after.append(list_before[size-idx-1])
  return list_after

class Blackjack():
  def __init__(self):
    self.player_sum=torch.LongTensor(np.arange(12,22,1)) #state: player's sum
    self.usable_ace=[0,1] #boolean for usable ace
    self.dealer_showing=torch.LongTensor(np.arange(1,11,1)) #one card dealer is showing
    self.state=get_state(self.player_sum, self.dealer_showing, self.usable_ace) #state of index
    self.action_list=[0,1] #0 indicating hit, 1 indicating stick
    self.len_a=len(self.action_list)

    self.target_policy=get_init_policy(self.state, self.player_sum) #target_policy tensor based on state_index
    self.behavior_policy=get_behavior_policy(self.state, self.player_sum, self.action_list)

    self.action_value=torch.zeros(len(self.state), len(self.action_list))
    self.state_value=torch.zeros(len(self.state))
    self.av_count=torch.zeros(len(self.state), len(self.action_list))
    self.sv_count=torch.zeros(len(self.state))

    self.cumulative_isr=torch.zeros(len(self.state), len(self.action_list))
    self.cumulative_isr_s=torch.zeros(len(self.state))
    self.iter=10000
    self.gamma=1.


  def find_state_idx(self, state):
    for idx, st in enumerate(self.state):
      if st[0]==state[0] and st[1]==state[1] and st[2]==state[2]:
        return idx

  def progression(self, sidx, aidx): #progression of episode following behavior policy
    current=self.state[sidx]
    p_idx=current[0]
    d_idx=current[1]
    u_idx=current[2]
    if aidx==0: #hit
      next_card=np.random.randint(1,11) #if A is next card-> always count as 1
      player=self.player_sum[p_idx]+next_card
      if player<=21: #normal progression
        p_idx_f=player-12
        termination=False
        reward=0
      else: #goes bust
        if self.usable_ace[u_idx]: #if usable ace is available
          u_idx=0 #usable ace is no longer functionate
          player-=10 #convert ace from 11 to 1
          p_idx_f=player-12
          termination=False
          reward=0
        else:
          p_idx_f=p_idx
          termination=True 
          reward=-1
    else: #stick w/o bust
      p_idx_f=p_idx
      termination=True
      reward=self.dealer_progression(p_idx_f, d_idx)
    state_f=[p_idx_f, d_idx, u_idx]
    state_f_idx=self.find_state_idx(state_f)
    return state_f_idx, reward, termination
  
  def dealer_progression(self, p_idx_f, d_idx): #case of non-bust termination
    player=self.player_sum[p_idx_f].detach().clone()
    dealer=self.dealer_showing[d_idx].detach().clone()
    while(1): #dealer's action
      next_card=np.random.randint(1,11)
      dealer+=next_card
      if dealer>=17: #hit until 17
        break
    #deciding reward
    if dealer>21: #case of dealer bust
      reward=1
    else:
      if player>dealer:
        reward=1
      elif player==dealer:
        reward=0
      else:
        reward=-1
    return reward
  
  def episode_generator(self): #following policy
    episode_sidx=[]
    episode_aidx=[]
    episode_reward=[]
    #fixed initial state
    init_sidx=self.find_state_idx([1,1,1])
    episode_sidx.append(init_sidx)
    init_aidx=random.choices(self.action_list, self.behavior_policy[init_sidx])
    episode_aidx.append(int(init_aidx[0]))
    #generate episode until termination
    termination=False
    while(termination==False):
      sidx=episode_sidx[-1] #prev. state
      aidx=episode_aidx[-1] #prev. action
      state_f_idx, reward, termination=self.progression(sidx, aidx)
      a_idx_f=random.choices(self.action_list, self.behavior_policy[state_f_idx]) #action based on behavior policy
      episode_reward.append(reward)
      if termination==False:
        episode_sidx.append(state_f_idx)
        episode_aidx.append(int(a_idx_f[0]))
    return episode_sidx, episode_aidx, episode_reward
  
  def evaluation(self):
    for epoch in range(1, self.iter+1):
      ep_sidx, ep_aidx, ep_rew=self.episode_generator()
      ep_sidx=reverse_list(ep_sidx) #access data in reversing order
      ep_aidx=reverse_list(ep_aidx)
      ep_rew=reverse_list(ep_rew)
      returns=0
      isr=1
      isr_s=1
      for idx, s_idx in enumerate(ep_sidx):
        a_idx=ep_aidx[idx]
        reward=ep_rew[idx]
        returns+=reward
        isr_s=isr_s/self.behavior_policy[s_idx][a_idx]
        self.cumulative_isr[s_idx,a_idx]+=isr
        self.cumulative_isr_s[s_idx]+=isr_s
        self.state_value[s_idx]=self.state_value[s_idx]+isr_s*(returns-self.state_value[s_idx])/self.cumulative_isr_s[s_idx]
        self.action_value[s_idx, a_idx]=self.action_value[s_idx, a_idx]+isr*(returns-self.action_value[s_idx, a_idx])/self.cumulative_isr[s_idx, a_idx] #update on average
        isr=isr/self.behavior_policy[s_idx][a_idx] #exclude action at current state (given (s,a) pair)
    return self.action_value, self.state_value

blackjack=Blackjack()
state=[1,1,1]
s_idx=blackjack.find_state_idx(state)
value=0
count=0
for _ in range(20):
  action_value, state_value=blackjack.evaluation()
  count+=1
  value=value+(state_value[s_idx]-value)/count
print(value)