# -*- coding: utf-8 -*-
"""RL: mountain car: et

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W-xBDIobJOIlm43MCMztWG67PynD_57Q
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
import random

import json
import pandas as pd
import os.path
import xml.etree.ElementTree as ET
import PIL
from google.colab import drive

import itertools
from itertools import product

torch.manual_seed(0)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

def get_tiling_points(start, tile_length, bound):
  tiling_points=[]
  left_bound=bound[0]
  right_bound=bound[1]
  if start>left_bound:
    tiling_points.append(left_bound)
    tiling_points.append(start)
  else:
    tiling_points.append(start)
  count=1
  while True:
    point=start+count*tile_length
    if point>=right_bound:
      tiling_points.append(right_bound)
      break
    else:
      tiling_points.append(point)
      count+=1
  return tiling_points

def get_tiling_grids(tiling_points_dim):
  size=len(tiling_points_dim)
  tiling_grids_dim=torch.zeros(size-1, 2).to(device)
  for idx in range(size-1):
    left=tiling_points_dim[idx]
    right=tiling_points_dim[idx+1]
    tiling_grids_dim[idx,:]=torch.FloatTensor([left, right])
  return tiling_grids_dim

def combine_tiling_grids(tiling_grids):
  tg_list=[]
  for tiling_grid in tiling_grids:
    tg_list.append(tiling_grid.cpu().numpy())
  tiling=list(itertools.product(*list(tg_list)))
  tiling=torch.FloatTensor(tiling).to(device)
  return tiling

def find_idx(state, tiling):
  for idx, grid in enumerate(tiling):
    n_dim=grid.size(0)
    flags=torch.zeros(n_dim).long().to(device)
    for dim in range(n_dim):
      left=grid[dim,0]
      right=grid[dim,1]
      if state[dim]>=left and state[dim]<=right:
        flags[dim]=1
    if torch.sum(flags)==n_dim:
      return idx

#grid tile coding: binary coarse coding for continuous state-space
class Tile_Coding():
  def __init__(self, s_dim, bounds, n_tilings, n_tiles):
    self.s_dim=s_dim #dimension of the state-space
    self.bounds=bounds #bounds of each dimension, size=s_dim
    self.n_tilings=n_tilings #No. of tilings
    self.n_tiles=n_tiles #No. of tiles of each dimension
    self.tilings, self.n_features_list=self.get_tilings()
    self.n_features=torch.sum(self.n_features_list).item()
  
  def get_tilings(self):
    #for each tiling:
    tilings=[]
    n_features_list=torch.zeros(self.n_tilings).long().to(device)
    for tilings_idx in range(self.n_tilings):
      #for each dimension (state-space)
      tiling_grids=[]
      #get tiling grids for each dimension
      for dim in range(self.s_dim):
        #sizes of a tiling in each dimension
        bound=self.bounds[dim]
        n_tile=self.n_tiles[dim]
        bound_length=bound[1]-bound[0]
        tile_length=bound_length/n_tile
        offset_length=tile_length/self.n_tilings
        start=bound[0]+tilings_idx*offset_length
        #grid points of a tiling for each dimension
        tiling_points_dim=get_tiling_points(start, tile_length, bound)
        #grid foramt (left, right) of a tiling for each dimension
        tiling_grids_dim=get_tiling_grids(tiling_points_dim)
        tiling_grids.append(tiling_grids_dim)
      #using constructed grids-> construct tiling
      tiling=combine_tiling_grids(tiling_grids)
      tilings.append(tiling)
      n_features_list[tilings_idx]=tiling.size(0)
    return tilings, n_features_list
      
  #return feature vector for "state"
  #feature_vector for s,a -> feat_vector for s'? ->
  def get_feature_vector(self, state):
    idx_list=[]
    #find index in each tiling
    for tiling in self.tilings:
      idx=find_idx(state, tiling)
      idx_list.append(idx)
    #transform idx list into feature vector
    sum=0
    feature_vector=torch.zeros(self.n_features).to(device)
    for tilings_idx, idx in enumerate(idx_list):
      f_index=sum+idx
      feature_vector[f_index]=1
      sum+=(self.n_features_list[tilings_idx])
    return feature_vector

#Agent
class Car_Agent():
  def __init__(self):
    s_dim=2
    self.bounds=[[-1.2,0.5],[-0.07,0.07]]
    n_tiles=[8,8] #No. of tiles for each dimension
    n_tilings=8
    self.tile_coding=Tile_Coding(s_dim, bounds, n_tilings, n_tiles)
    self.n_features=self.tile_coding.n_features #dimension of feature_vectors and weights
    self.actions_list=[1,0,-1]
    self.a_idx_list=[0,1,2]
    self.len_a=len(self.actions_list)
    self.len_f=self.n_features+self.len_a
    self.weight=torch.zeros(self.len_f).to(device)
    self.weight_init()
  
  def weight_init(self):
    self.weight=torch.normal(torch.zeros(self.len_f).to(device), torch.ones(self.len_f).to(device)).to(device)
    return
  
  def progression(self, state, a_idx):
    loc_i=state[0]
    vel_i=state[1]
    action=self.actions_list[a_idx]
    #get final velocity first
    vel_f=vel_i+0.001*action-0.0025*np.cos(3*loc_i)
    #bound the velocity
    if vel_f>self.bounds[1][1]:
      vel_f=self.bounds[1][1]
    elif vel_f<self.bounds[1][0]:
      vel_f=self.bounds[1][0]
    #get final location
    loc_f=loc_i+vel_f
    #bound the location
    if loc_f>self.bounds[0][1]:
      loc_f=self.bounds[0][1]
    elif loc_f<self.bounds[0][0]:
      loc_f=self.bounds[0][0]
    #if final state crosses bounds
    if loc_f==self.bounds[0][1]:
      termination=True
    elif loc_f==self.bounds[0][0]:
      vel_f=0
      termination=False
    else:
      termination=False
    #final state
    state_f=[loc_f, vel_f]
    reward=-1
    return state_f, reward, termination
  
  def e_greedy_selection(self, state, eps):
    q_vector=torch.zeros(self.len_a).to(device)
    probability=[eps/self.len_a for _ in range(self.len_a)]
    for a_idx in self.a_idx_list:
      q_vector[a_idx]=self.get_action_value(state, a_idx)
    argmax_idx=torch.argmax(q_vector, dim=0)
    probability[argmax_idx]+=1-eps
    a_idx=random.choices(self.a_idx_list, probability)[0]
    return a_idx
  
  #binary feature vector
  def get_feature_vector(self, state, a_idx):
    feature_vector=torch.zeros(self.len_f).to(device)
    fv=self.tile_coding.get_feature_vector(state)
    feature_vector[:self.n_features]=fv
    feature_vector[self.n_features+a_idx]=1
    return feature_vector
  
  def get_action_value(self, state, a_idx):
    feature_vector=self.get_feature_vector(state, a_idx)
    action_value=torch.dot(feature_vector, self.weight)
    return action_value

#Training  Weight using Sarsa(lambda) algorithm
class Sarsa_lambd():
  def __init__(self):
    self.agent=Car_Agent()
    self.elig_trace=torch.zeros(self.agent.len_f).to(device)

  def train(self, iter, step_size, tr_decay_param, et_type):
    for epoch in range(1, iter+1):
      #initialize trace for each episode
      self.elig_trace=torch.zeros(self.agent.len_f).to(device)
      gamma=1.
      #initialize state, a_idx and storages
      loc_start=random.uniform(-0.6, -0.4)
      vel_start=0
      state=[loc_start, vel_start]
      a_idx=self.agent.e_greedy_selection(state, eps=0)
      ep_step=0
      while True:
        state_f, reward, termination=self.agent.progression(state, a_idx)
        fv=self.agent.get_feature_vector(state, a_idx)
        Q=self.agent.get_action_value(state, a_idx)
        if et_type=='accumulating':
          #update accumulating trace
          self.elig_trace=tr_decay_param*gamma*self.elig_trace+fv
        elif et_type=='replacing':
          #update replacing trace based on feature vector's component
          for z_idx, f_val in enumerate(fv):
            if f_val==1:
              self.elig_trace[z_idx]=1
            else:
              self.elig_trace[z_idx]=tr_decay_param*gamma*self.elig_trace[z_idx]
        else:
          print("Invalid eligibility trace type")
          break
        if termination:
          TD_error=reward-Q
          self.agent.weight=self.agent.weight+step_size*TD_error*self.elig_trace
          break
        else:
          a_idx_f=self.agent.e_greedy_selection(state_f, eps=0)
          Q_f=self.agent.get_action_value(state_f, a_idx_f)
          TD_error=reward+gamma*Q_f-Q
          self.agent.weight=self.agent.weight+step_size*TD_error*self.elig_trace
          state=state_f
          a_idx=a_idx_f
          ep_step+=1
      print("Epoch: {:d}, Episode Length: {:d}, Weight Vector Sum: {:.3f}".format(epoch, ep_step, torch.sum(self.agent.weight).item()))
    return self.agent.weight

class True_Online_Sarsa():
  def __init__(self):
    self.agent=Car_Agent()
  
  def train(self, iter, step_size, tr_decay_param):
    for epoch in range(1, iter+1):
      #initialize dutch trace per episode
      dutch_trace=torch.zeros(self.agent.len_f).to(device)
      gamma=1.
      #initialize state, a_idx and storages
      loc_start=random.uniform(-0.6, -0.4)
      vel_start=0
      state=[loc_start, vel_start]
      a_idx=self.agent.e_greedy_selection(state, eps=0)
      Q_old=0
      ep_step=0
      while True:
        state_f, reward, termination=self.agent.progression(state, a_idx)
        Q=self.agent.get_action_value(state, a_idx)
        a_idx_f=self.agent.e_greedy_selection(state_f, eps=0)
        Q_f=self.agent.get_action_value(state_f, a_idx_f)
        TD_error=reward+gamma*Q_f-Q
        fv=self.agent.get_feature_vector(state, a_idx)
        dutch_trace=tr_decay_param*gamma*dutch_trace+(1-step_size*tr_decay_param*gamma*torch.dot(dutch_trace, fv))*fv
        self.agent.weight=self.agent.weight+step_size*(TD_error+Q-Q_old)*dutch_trace-step_size*(Q-Q_old)*fv
        if termination:
          break
        else:
          Q_old=Q_f
          state=state_f
          a_idx=a_idx_f
          ep_step+=1
      print("Epoch: {:d}, Ep_step: {:d}, Weight vector sum: {:.3f}".format(epoch, ep_step, torch.sum(self.agent.weight).item()))
    return self.agent.weight

s_dim=2
bounds=[[-1.2,0.5],[-0.07,0.07]]
n_tiles=[8,8] #No. of tiles for each dimension
n_tilings=8

sarsa_lambd=Sarsa_lambd()
#elig trace type: accumulating or replacing
#acc_weight=sarsa_lambd.train(iter=100, step_size=0.1/8, tr_decay_param=0.9, et_type='accumulating')
rep_weight=sarsa_lambd.train(iter=100, step_size=0.1/8, tr_decay_param=0.9, et_type='replacing')

true_online_sarsa=True_Online_Sarsa()
weight=true_online_sarsa.train(iter=100, step_size=0.1/8, tr_decay_param=0.9)