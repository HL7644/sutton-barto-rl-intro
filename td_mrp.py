# -*- coding: utf-8 -*-
"""TD: MRP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FfmkBGm0vr9tOi4KTlLx6tXb5woG19iM
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
import random

torch.manual_seed(0)

class random_walk():
  def __init__(self):
    self.state=[0,1,2,3,4] #each corresponding to A,B,C,D,E
    self.len_s=len(self.state)
    self.s_idx_list=np.arange(0,self.len_s,1)
    self.actions=[-1,1] #left, right
    self.len_a=len(self.actions)
    self.a_idx_list=np.arange(0,self.len_a,1)
    self.state_value=torch.full([self.len_s], 0.5)
    self.action_value_sarsa=torch.full((self.len_s, self.len_a), 0.5)
    self.action_value_q=torch.full((self.len_s, self.len_a), 0.5)
    self.action_value_exp_sarsa=torch.full((self.len_s, self.len_a), 0.5)
    self.behavior_policy=self.get_behavior_policy()
    self.iter=1000
    self.step_size=0.02
  
  def get_behavior_policy(self):
    policy=[]
    for s_idx, state in enumerate(self.s_idx_list):
      policy.append([0.5, 0.5]) #equiprobable
    return policy
  
  def progression(self, s_idx, a_idx):
    action=self.actions[a_idx]
    s_idx_f=s_idx+action
    if s_idx_f<0:
      termination=True
      reward=0
    elif s_idx_f>=self.len_s:
      termination=True
      reward=1
    else:
      termination=False
      reward=0
    return s_idx_f, reward, termination

  def episode_generator(self):
    ep_state=[]
    ep_action=[]
    ep_reward=[]
    ep_state.append(2) #start at C
    ep_action.append(random.choice(self.a_idx_list))
    termination=False
    while termination==False:
      s_idx=ep_state[-1]
      a_idx=ep_action[-1]
      s_idx_f, reward, termination=self.progression(s_idx, a_idx)
      ep_reward.append(reward)
      if termination==False:
        ep_state.append(s_idx_f)
        a_idx_f=random.choice(self.a_idx_list)
        ep_action.append(a_idx_f)
    return ep_state, ep_action, ep_reward

  def evaluation(self):
    for epoch in range(1, self.iter+1):
      ep_s, ep_a, ep_r=self.episode_generator()
      for idx, s_idx in enumerate(ep_s):
        a_idx=ep_a[idx]
        reward=ep_r[idx]
        if (idx+1)==len(ep_s):
          self.state_value[s_idx]=self.state_value[s_idx]+self.step_size*(reward-self.state_value[s_idx]) #terminal value=0
          #sarsa
          self.action_value_sarsa[s_idx, a_idx]+=self.step_size*(reward-self.action_value_sarsa[s_idx, a_idx])
          #q-learning
          self.action_value_q[s_idx, a_idx]+=self.step_size*(reward-self.action_value_q[s_idx, a_idx])
          #exp-sarsa
          self.action_value_exp_sarsa[s_idx,a_idx]+=self.step_size*(reward-self.action_value_exp_sarsa[s_idx, a_idx])
          break
        else:
          s_idx_f=ep_s[idx+1]
          a_idx_f=ep_a[idx+1]
          self.state_value[s_idx]=self.state_value[s_idx]+self.step_size*(reward+self.state_value[s_idx_f]-self.state_value[s_idx])
          #sarsa
          self.action_value_sarsa[s_idx, a_idx]+=self.step_size*(reward+self.action_value_sarsa[s_idx_f, a_idx_f]-self.action_value_sarsa[s_idx,a_idx])
          #q-learning
          temp=self.action_value_q[s_idx_f,:]
          argmax_idx=torch.argmax(temp, dim=0)
          self.action_value_q[s_idx,a_idx]+=self.step_size*(reward+self.action_value_q[s_idx_f, argmax_idx]-self.action_value_q[s_idx, a_idx])
          #exp-sarsa
          expected=0
          policy_temp=self.behavior_policy[s_idx_f]
          for a_idx_temp in self.a_idx_list:
            expected+=self.action_value_exp_sarsa[s_idx_f, a_idx_temp]*policy_temp[a_idx_temp]
          self.action_value_exp_sarsa[s_idx,a_idx]+=self.step_size*(reward+expected-self.action_value_exp_sarsa[s_idx,a_idx])
          
    return self.state_value, self.action_value_sarsa, self.action_value_q, self.action_value_exp_sarsa

mrp=random_walk()
sv, av_sarsa, av_q, av_exp_sarsa=mrp.evaluation()

print(av_sarsa)
print(av_q)
print(av_exp_sarsa)