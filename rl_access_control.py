# -*- coding: utf-8 -*-
"""RL: Access Control

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1T1FOtGeaHp1iU3E_NzP7M66tiFsbH35k
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
import random

import json
import pandas as pd
import os.path
import xml.etree.ElementTree as ET
import PIL
from google.colab import drive

import itertools
from itertools import product

torch.manual_seed(0)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

def get_states(servers, prior_list): #for binary state vector
  server_state=[0,1] #0: empty, 1: occupied
  servers_state=[server_state for _ in servers]
  states=list(product(*list(servers_state), prior_list))
  states=torch.FloatTensor(states).to(device)
  return states

def gs(n_empty_list, cust_prior): #just indicating empty servers and priority
  states=list(itertools.product(n_empty_list, cust_prior))
  return states

class AC():
  def __init__(self, k, p):
    self.k=k #No. of servers
    self.p=p
    self.n_empty_list=np.arange(0, self.k+1, 1)
    self.cust_prior=[1,2,4,8]
    self.len_prior=len(self.cust_prior)
    #states: #of empty servers + priority
    self.states=gs(self.n_empty_list, self.cust_prior)
    self.len_s=len(self.states)
    self.a_idx_list=[0,1] #0 means reject, 1 means accept
    self.len_a=len(self.a_idx_list)
    #feature vector dimension
    self.dim_f=(self.k+1)+1+self.len_a #ll+4

    #ftn approx
    self.weight=torch.full([self.dim_f], 0.).to(device)
    self.avg_reward=torch.FloatTensor([0]).to(device)
    #values: use tabular
    self.action_values=torch.full([self.len_s, self.len_a], 2.).to(device)
    self.count=torch.zeros(self.len_s).to(device)
  
  def find_s_idx(self, state):
    for s_idx, st in enumerate(self.states):
      if state[0]==st[0] and state[1]==st[1]:
        return s_idx
  
  def get_feature_vector(self, state, a_idx):
    feature_vector=torch.zeros(self.dim_f).to(device)
    n_empty=state[0]
    priority=state[1]
    feature_vector[n_empty]=1
    #priority
    feature_vector[self.k+1]=priority
    feature_vector[self.k+1+1+a_idx]=1 #action
    return feature_vector
  
  def get_action_value(self, state, a_idx):
    feature_vector=self.get_feature_vector(state, a_idx)
    action_value=torch.dot(self.weight, feature_vector)
    return action_value

  def e_greedy_selection(self, state, eps):
    n_empty=state[0]
    priority=state[1]
    s_idx=self.find_s_idx(state)
    if n_empty==0:
      selected_idx=0 #reject: 0
    else:
      q_vector=torch.zeros(self.len_a).to(device)
      prob=[eps/self.len_a for _ in range(self.len_a)]
      for a_idx in self.a_idx_list:
        q_vector[a_idx]=self.action_values[s_idx, a_idx]
      argmax_idx=torch.argmax(q_vector, dim=0)
      prob[argmax_idx]+=1-eps
      selected_idx=random.choices(self.a_idx_list, prob)[0]
    return selected_idx

  def progression(self, state, a_idx):
    n_empty=state[0]
    priority=state[1]
    state_f=[0,0]
    if a_idx==0: #reject
      n_empty_f=n_empty
      reward=0
    else:
      n_empty_f=n_empty-1 #assign to server
      reward=priority
    #free servers
    n_occupied=self.k-n_empty_f
    for server in range(n_occupied):
      pr=random.random()
      if pr<self.p:
        n_empty_f+=1
    next_priority=random.choice(self.cust_prior)
    state_f=[n_empty_f, next_priority]
    return state_f, reward

  def train(self, N_steps, avgr_step_size, TD_step_size):
    state=[self.k, 1] #all empty servers with priority 1
    a_idx=1 #initially accept
    for step in range(1, N_steps+1):
      state_f, reward=self.progression(state, a_idx)
      s_idx=self.find_s_idx(state)
      self.count[s_idx]+=1
      s_idx_f=self.find_s_idx(state_f)
      a_idx_f=self.e_greedy_selection(state_f, eps=0.1)
      #print("Step: {:d}".format(step))
      #print(s_idx, a_idx, s_idx_f, a_idx_f, reward)
      Q=self.action_values[s_idx, a_idx]
      Q_f=self.action_values[s_idx_f, a_idx_f]
      TD_error=reward-self.avg_reward+Q_f-Q
      self.avg_reward=self.avg_reward+avgr_step_size*TD_error
      #feature_vector=self.get_feature_vector(state, a_idx)
      self.action_values[s_idx, a_idx]=self.action_values[s_idx, a_idx]+TD_step_size*TD_error
      state=state_f
      a_idx=a_idx_f
      if step%(N_steps/20)==0:
        print("Step: {:d}, Avg_Reward: {:.4f}, Q Sum: {:.3f}".format(step, self.avg_reward.item(), torch.sum(self.action_values).item()))
    return self.action_values, self.count
  
  def get_optimal_policy(self):
    optimal_policy=torch.full([(self.k+1), self.len_prior], 0.).to(device)
    for s_idx, state in enumerate(self.states):
      n_empty=state[0]
      priority=state[1]
      p_idx=int(np.log2(priority))
      if n_empty==0:
        argmax_idx=0
      else:
        q_vector=torch.zeros(self.len_a).to(device)
        for a_idx in self.a_idx_list:
          q_vector[a_idx]=self.action_values[s_idx, a_idx]
        argmax_idx=torch.argmax(q_vector, dim=0)
      optimal_policy[n_empty, p_idx]=argmax_idx
    return optimal_policy

k=10
p=0.06

ac=AC(k,p)
Qs, count=ac.train(N_steps=2000000, avgr_step_size=1e-2, TD_step_size=1e-2)
print(Qs)
print(count)

weight=ac.train(N_steps=40, avgr_step_size=1e-2, TD_step_size=1e-2)

optimal_policy=ac.get_optimal_policy()
print(optimal_policy)

class Access_Control():
  def __init__(self, k, p):
    self.k=k
    self.p=p
    self.servers=np.arange(0,self.k,1)
    self.cust_prior=[1,2,4,8]
    self.len_prior=len(self.cust_prior)
    self.a_idx_list=[0,1] #0 means reject, 1 means accept
    self.len_a=len(self.a_idx_list)
    #features contain servers, norm_prior, a_idx_list-> normalize features to [0,1]
    self.states=get_states(self.servers, self.cust_prior)
    self.len_s, self.dim_s=self.states.size()
    #dim. for feature vector
    self.dim_f=self.k+self.len_prior+self.len_a #server+priority+actions

    #ftn approx
    self.weight=torch.zeros(self.dim_f).to(device)
    #tabular
    self.action_values=torch.zeros(self.len_s, self.len_a)
    self.avg_reward=torch.FloatTensor([0]).to(device)
    
  def get_feature_vector(self, state, a_idx):
    servers=state[:self.k]
    priority=state[-1]
    feature_vector=torch.zeros(self.dim_f).to(device)
    #servers
    feature_vector[:self.k]=servers
    #priority-> binary
    p_idx=int(torch.log2(priority))
    feature_vector[self.k+p_idx]=1
    #actions
    feature_vector[self.k+self.len_prior+a_idx]=1
    return feature_vector
    
  def get_action_value(self, state, a_idx):
    feature_vector=self.get_feature_vector(state, a_idx)
    action_value=torch.dot(self.weight, feature_vector)
    return action_value

  def e_greedy_selection(self, state, eps):
    servers=state[:self.k]
    if torch.sum(servers)==self.k:
      a_idx=0 #must reject
    else:
      q_vector=torch.zeros(self.len_a).to(device)
      probability=[eps/self.len_a for _ in range(self.len_a)]
      for a_idx in self.a_idx_list:
        q_vector[a_idx]=self.get_action_value(state, a_idx)
      argmax_idx=torch.argmax(q_vector, dim=0)
      probability[argmax_idx]+=1-eps
      a_idx=random.choices(self.a_idx_list, probability)[0]
    return a_idx

  def progression(self, state, a_idx):
    servers=state[:self.k]
    priority=state[-1]
    state_f=state.clone().detach()
    if a_idx==0: #rejected
      reward=0
      assigned_idx=2*self.k
    else: #accepted
      #assign to earliest server (only 1 server)
      for idx, server in enumerate(state_f):
        if server==0: #if server is available
          assigned_idx=idx
          state_f[idx]=1 #occupy
          reward=priority
          break
    #free server
    for idx, server in enumerate(state_f):
      if idx!=assigned_idx and server==1:
        prob=random.random()
        if prob<self.p:
          state_f[idx]=0 #freed
    #determine next priority
    next_priority=random.choice(self.cust_prior)
    state_f[-1]=next_priority
    return state_f, reward

  def ftn_train(self, N_steps, avgr_step_size, TD_step_size):
    state=torch.zeros(self.k+1).to(device) #servers
    state[-1]=1 #priority
    for step in range(1, N_steps+1):
      #print(state)
      servers=state[:self.k]
      a_idx=self.e_greedy_selection(state, eps=0.1)
      state_f, reward=self.progression(state, a_idx)
      a_idx_f=self.e_greedy_selection(state_f, eps=0.1)
      Q=self.get_action_value(state, a_idx)
      Q_f=self.get_action_value(state_f, a_idx_f)
      TD_error=reward-self.avg_reward+Q_f-Q
      self.avg_reward=self.avg_reward+avgr_step_size*TD_error
      feature_vector=self.get_feature_vector(state, a_idx)
      #print(feature_vector)
      self.weight=self.weight+TD_step_size*TD_error*feature_vector
      state=state_f
      a_idx=a_idx_f
      if step%(N_steps/20)==0:
        print("Step: {:d}, Avgr: {:.3f}, Weight Vector Sum: {:.3f}".format(step, self.avg_reward.item(), torch.sum(self.weight).item()))
    return self.weight
  
  def get_action_values(self):
    return 
  
  def test(self):
    optimal_policy=torch.zeros(self.k+1, self.len_prior).to(device) #could be 0-k empty servers
    count=torch.full([self.k+1, self.len_prior], 1).to(device)
    for state in self.states:
      servers=state[:self.k]
      prior=state[-1]
      p_idx=int(torch.log2(prior))
      n_servers=torch.sum(servers).item()
      n_empty=int(self.k-n_servers)
      if n_empty==0:
        argmax_idx=0
      else:
        argmax_idx=self.e_greedy_selection(state, eps=0)
      optimal_policy[n_empty, p_idx]+=(argmax_idx-optimal_policy[n_empty,p_idx])/count[n_empty,p_idx]
      count[n_empty, p_idx]+=1
  
    return optimal_policy

k=10
p=0.06

access_control=Access_Control(k,p)
weight=access_control.ftn_train(N_steps=2000000, avgr_step_size=1e-2, TD_step_size=1e-2)

print(weight)

optimal_policy=access_control.test()
print(optimal_policy)

k=10
p=0.06

access_control=Access_Control(k,p)
optimizer=optim.SGD(access_control.parameters(), lr=1e-2)

iter=200000
#initialize (s,a)
state=torch.zeros(11).to(device)
state[-1]=1/8
a_idx=1 #accept initially
for epoch in range(1, iter+1):
  q=access_control(state, a_idx)
  state_f, reward=access_control.progression(state, a_idx)
  servers_f=state_f[:k]
  if torch.sum(servers_f).item()==k:
    a_idx_f=0
  else:
    a_idx_f=access_control.e_greedy_selection(state_f, eps=0.1)
  q_f=access_control(state_f, a_idx_f)
  avgr=access_control.avg_reward
  cost=(reward-avgr+q_f-q)**2 #target: reward-avgr+q_f, current value: q

  optimizer.zero_grad()
  cost.backward()
  optimizer.step()

  state=state_f
  a_idx=a_idx_f

  if epoch%(iter/20)==0:
    action_values=access_control.get_action_values()
    avg_reward=access_control.avg_reward
    print("Epoch: {:d}, Sum of Q: {:3f}, Avg Reward: {:.3f}".format(epoch, torch.sum(action_values).item(), avg_reward.item()))
action_values=access_control.get_action_values()
avg_reward=access_control.avg_reward

optimal_policy=access_control.test()

print(action_values[1])

print(optimal_policy)

