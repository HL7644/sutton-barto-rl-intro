# -*- coding: utf-8 -*-
"""RL: Gradient TD

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19K6_EngUU7yqgfuVJRwwfYyDRKx0EIvE
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
import random

import json
import pandas as pd
import os.path
import xml.etree.ElementTree as ET
import PIL
from google.colab import drive

import itertools
from itertools import product

torch.manual_seed(0)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

class Baird():
  def __init__(self):
    self.s_idx_list=np.arange(0,7,1)
    self.a_idx_list=[0,1] #0: dashed, 1: soild
    self.len_s=len(self.s_idx_list)
    self.len_a=len(self.a_idx_list)
    self.d=8 #dimension of feature vector

    #weight
    self.weight=torch.full([8], 1.).to(device)
    self.v_vector=torch.zeros(8).to(device)
    #on-policy distribution
    self.op_dist=self.get_op_distribution()
    self.weight_init()

  def weight_init(self):
    #initialize weight
    self.weight=torch.full([8], 1.).to(device)
    self.weight[6]=10
    #initialize v_vector
    self.v_vector=torch.zeros(8).to(device)
    return
  
  #return probability and accordingly selected a_idx
  def target_policy(self, s_idx):
    policy=[0,1] #only takes solid actions
    a_idx=random.choices(self.a_idx_list, policy)[0]
    return policy, a_idx

  def behavior_policy(self, s_idx):
    policy=[6/7, 1/7] #dasehd: 6/7, solid: 1/7
    a_idx=random.choices(self.a_idx_list, policy)[0]
    return policy, a_idx
  
  def get_feature_vector(self, s_idx):
    feature_vector=torch.zeros(8).to(device)
    if s_idx==6:
      feature_vector[7]=2
      feature_vector[s_idx]=1
    else:
      feature_vector[7]=1
      feature_vector[s_idx]=2
    return feature_vector
  
  def get_state_value(self, s_idx):
    feature_vector=self.get_feature_vector(s_idx)
    state_value=torch.dot(self.weight, feature_vector)
    return state_value

  def progression(self, s_idx, a_idx):
    upper_s_idx=np.arange(0,6,1)
    if a_idx==0: #dashed move
      #randomly to upper 6 states
      s_idx_f=random.choice(upper_s_idx)
      reward=0
    else: #soild move
      s_idx_f=6 #moves to lower state
      reward=0
    return s_idx_f, reward
  
  def get_op_distribution(self): #mu(s) w.r.t behvaior policy
    s_idx=0
    count=torch.zeros(self.len_s)
    for step in range(100000):
      count[s_idx]+=1
      _,a_idx=self.behavior_policy(s_idx)
      s_idx_f, _=self.progression(s_idx, a_idx)
      s_idx=s_idx_f
    op_dist=count/torch.sum(count)
    return op_dist
    
  def TDC(self, prim_steps, prim_step_size, sec_step_size):
    gamma=.99
    self.weight_init()
    #primary learning to train weight
    print("Primary Learning: Weight")
    s_idx=0
    b_policy, a_idx=self.behavior_policy(s_idx)
    for prim_step in range(1, prim_steps+1):
      t_policy,_=self.target_policy(s_idx)
      isr=t_policy[a_idx]/b_policy[a_idx]
      s_idx_f, reward=self.progression(s_idx, a_idx)

      V=self.get_state_value(s_idx)
      V_f=self.get_state_value(s_idx_f)
      TD_error=reward+gamma*V_f-V
      fv=self.get_feature_vector(s_idx)
      fv_f=self.get_feature_vector(s_idx_f)
      self.v_vector+=sec_step_size*isr*(TD_error-torch.dot(self.v_vector, fv))*fv
      self.weight+=prim_step_size*isr*(TD_error*fv-gamma*torch.dot(fv, self.v_vector)*fv_f)
      s_idx=s_idx_f
      b_policy, a_idx=self.behavior_policy(s_idx_f)
      if prim_step%(prim_steps/20)==0:
        ve=self.get_VE()
        pbe=self.get_PBE().squeeze(dim=0)
        print("Step: {:d}, Weight vector sum: {:.4f}".format(prim_step, torch.sum(self.weight).item()))
        print("VE: {:.3f}, PBE: {:.3f}".format(ve, pbe.item()))
    return self.v_vector, self.weight
  
  def get_PBE(self):
    gamma=.99
    X=torch.zeros(self.len_s, self.d).to(device)
    D=torch.zeros(self.len_s, self.len_s).to(device)
    BE_vector=torch.zeros(self.len_s, 1).to(device)
    #Construct X, D, BE Matrix
    for idx, s_idx in enumerate(self.s_idx_list):
      fv=self.get_feature_vector(s_idx)
      X[idx,:]=fv #row as feature vectors
      D[idx, idx]=self.op_dist[idx] #diagonals as on-policy distribution
      #get BE for each state
      BE=0
      t_prob,_=self.target_policy(s_idx)
      #for target policy
      for a_idx, prob in enumerate(t_prob):
        #deteriministic environment
        s_idx_f, reward=self.progression(s_idx, a_idx)
        BE+=prob*(reward+gamma*self.get_state_value(s_idx_f))
      BE=BE-self.get_state_value(s_idx)
      BE_vector[idx,0]=BE
    #Calculate PBE
    first=(X.permute(1,0).matmul(D).matmul(BE_vector)).permute(1,0)
    second=torch.linalg.pinv(X.permute(1,0).matmul(D).matmul(X))
    third=X.permute(1,0).matmul(D).matmul(BE_vector)
    #print(first, second, third)
    pbe=first.matmul(second).matmul(third)
    return pbe
  
  def get_VE(self):
    ve=0
    for s_idx in self.s_idx_list:
      ve+=self.op_dist[s_idx]*(0-self.get_state_value(s_idx))**2
    return ve

baird=Baird()
#v=baird.TDC_secondary(1000, 5e-2)
v, w=baird.TDC(1000, 5e-3, 5e-2)

print(w)