# -*- coding: utf-8 -*-
"""TD:  Windy Gridworld

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SoIkxTm5q6k9IFrxkdJYhRV5uDDJ5zqw
"""

import torch
import torch.optim as optim
import torch.nn.functional as F
import torch.nn as nn
import torchvision.datasets as dsets
import numpy as np
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
import torchvision
import matplotlib.pyplot as plt
from mpl_toolkits import mplot3d
import random

torch.manual_seed(0)

def get_wind(grid):
  grid[:,3:6]=-1
  grid[:,6:8]=-2
  grid[:,8]=-1
  return grid

def get_state(grid):
  h,w=grid.size()
  states=[]
  for row in range(h):
    for col in range(w):
      states.append([row, col])
  return states

class windy_gridworld():
  def __init__(self, action_list):
    #get states & wind
    self.grid=torch.zeros(7,10) #grid cells
    self.states=get_state(self.grid)
    self.len_s=len(self.states)
    self.wind=get_wind(self.grid) #upwards wind
    self.start=[3,0]
    self.start_idx=self.find_s_idx(self.start)
    self.goal=[3,7]
    self.goal_idx=self.find_s_idx(self.goal)
    #get actions
    self.action_list=action_list #each corresponding to up, down, left, right
    self.len_a=len(self.action_list)
    self.avail_actions_idx=self.get_avail_action_idx()
    #values
    self.action_value=torch.zeros(self.len_s, self.len_a)
    #iteration parameters
    self.epsilon=0.1
    self.iter=10000
    self.step_size=0.1
    self.gamma=1.


  def get_avail_action_idx(self):
    avail_actions_idx=[]
    for st in self.states:
      temp=[]
      wind=self.wind[st[0],st[1]]
      for a_idx, a in enumerate(self.action_list):
        if a[0]==0 and a[1]==0:
          if wind!=0:
            temp.append(a_idx)
        else:
          st_f0=st[0]+a[0]
          st_f1=st[1]+a[1]
          if st_f0>=0 and st_f1>=0 and st_f0<7 and st_f1<10:
            temp.append(a_idx)
      avail_actions_idx.append(temp)
    return avail_actions_idx

  def find_s_idx(self, state):
    for s_idx, st in enumerate(self.states):
      if state[0]==st[0] and state[1]==st[1]:
        return s_idx
  
  def progression(self, s_idx, a_idx):
    state=self.states[s_idx]
    action=self.action_list[a_idx]
    wind=self.wind[state[0],state[1]]
    sf0=state[0]+action[0]+wind
    sf1=state[1]+action[1]
    if sf0<=0: #wind cannot move agent out of grid
      sf0=0
    state_f=[sf0,sf1]
    s_idx_f=self.find_s_idx(state_f)
    if s_idx_f==self.goal_idx:
      termination=True
      reward=-1
    else:
      termination=False
      reward=-1
    return s_idx_f, reward, termination

  def get_e_greedy(self, s_idx):
    av_action_idx=self.avail_actions_idx[s_idx]
    len_av=len(av_action_idx)
    e_greedy=[self.epsilon/len_av]*len_av
    temp=torch.zeros(len_av)
    for idx, a_idx in enumerate(av_action_idx):
      temp[idx]=self.action_value[s_idx,a_idx]
    argmax_idx=torch.argmax(temp, dim=0)
    e_greedy[argmax_idx]+=(1-self.epsilon)
    return e_greedy

  def sarsa(self):
    for epoch in range(1, self.iter+1):
      s_idx=self.start_idx
      termination=False
      while termination==False:
        e_greedy=self.get_e_greedy(s_idx)
        a_idx=random.choices(self.avail_actions_idx[s_idx], e_greedy)[0] #action choice w.r.t e-greedy policy
        s_idx_f, reward, termination=self.progression(s_idx, a_idx)
        e_greedy_f=self.get_e_greedy(s_idx_f)
        a_idx_f=random.choices(self.avail_actions_idx[s_idx_f], e_greedy_f)[0] #action_f choice w.r.t e-greedy policy
        #sarsa update
        self.action_value[s_idx,a_idx]+=self.step_size*(reward+self.gamma*self.action_value[s_idx_f, a_idx_f]-self.action_value[s_idx, a_idx])
        #change (s,a)
        s_idx=s_idx_f
        a_idx=a_idx_f
    return self.action_value

  def get_trajectory(self):
    #follow greedy w.r.t action value for optimal trajectory
    state_list=[]
    sidx_list=[]
    sidx_list.append(self.start_idx)
    termination=False
    while termination==False:
      s_idx=sidx_list[-1]
      av_action_idx=self.avail_actions_idx[s_idx]
      greedy=self.get_e_greedy(s_idx)
      argmax_idx=np.argmax(greedy)
      a_idx=av_action_idx[argmax_idx]
      s_idx_f, _, termination=self.progression(s_idx, a_idx)
      sidx_list.append(s_idx_f)
    traj_grid=torch.zeros(7,10)
    for s_idx in sidx_list:
      state=self.states[s_idx]
      traj_grid[state[0],state[1]]=1
      state_list.append(self.states[s_idx])
    return state_list, traj_grid

action_list_9=[[-1,0],[1,0],[0,-1],[0,1],[-1,-1],[-1,1],[1,-1],[1,1],[0,0]]
action_list_king=[[-1,0],[1,0],[0,-1],[0,1],[-1,-1],[-1,1],[1,-1],[1,1]]
action_list_normal=[[-1,0],[1,0],[0,-1],[0,1]]
gw_normal=windy_gridworld(action_list_normal)
gw_king=windy_gridworld(action_list_king)
gw_9=windy_gridworld(action_list_9)

action_value_normal=gw_normal.sarsa()
optimal_state_list_normal, optimal_traj_normal=gw_normal.get_trajectory()

action_value_king=gw_king.sarsa()
optimal_state_list_king, optimal_traj_king=gw_king.get_trajectory()

action_value_9=gw_9.sarsa()
optimal_state_list_9, optimal_traj_9=gw_9.get_trajectory()

print(len(optimal_state_list_normal))
print(optimal_traj_normal)

print(len(optimal_state_list_king))
print(optimal_traj_king)

print(len(optimal_state_list_9))
print(optimal_traj_9)

